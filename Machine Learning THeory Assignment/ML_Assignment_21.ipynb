{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRxtY6gummrE"
   },
   "source": [
    "### 1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?\n",
    "Ans>> The general formula is for depth of decision tree is `log2(m)^3`, where `m` is the number of instances,  and here with 1 million data set the depth will be log2(10^6) = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sc4En8yqmwcF"
   },
   "source": [
    "### 2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?\n",
    "Ans>> It is lower than itâ€™s parents, if one child is smaller than the other, it is possible for it to have a higher gini score than its parrent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "va01wRN-my8V"
   },
   "source": [
    "### 3. Explain if its a good idea to reduce max depth if a Decision Tree is overfitting the training set?\n",
    "Ans>> If the our decision tree is overfitting then we should reduce the max-depth for the algorithm. Because by reducing the max-depth it will be less variance and regulrarize the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmDxHV4qm2xt"
   },
   "source": [
    "### 4. Explain if its a  good idea to try scaling the input features if a Decision Tree underfits the training set?\n",
    "Ans>> The tree based model is not required scaling of the input features, so there will be no effect on model underfitting or overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yAqn86Vm502"
   },
   "source": [
    "### 5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances?\n",
    "Ans>> We calculate the traning time of decision tree algorithms is by using this formula.\n",
    "`Traning time = n * 10m * log(10m) / n * m * log(m)`, where `n` is the time and `m` is the instance number. In this case after calculation it will be 11.677 hours.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_a8FJTmm8o-"
   },
   "source": [
    "### 6. Will setting presort=True speed up training if your training set has 100,000 instances?\n",
    "Ans>> The `presort = True` is work when the dataset is smaller a like under few thousand instance, and this case the data set size is 100,000, s here it will make the algorithms slow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ivHlELTm-1O"
   },
   "source": [
    "### 7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zl2D6xaZnESn"
   },
   "source": [
    "#### a. To build a moons dataset, use make moons(n samples=10000, noise=0.4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEfYEqRLniX_",
    "outputId": "6fc26ef8-76d1-4a9f-80b7-f1ba4b068d6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 2), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# creating moon data\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.4)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_upGDKQpBDS"
   },
   "source": [
    "#### b. Divide the dataset into a training and a test collection with train test split()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUoQdep6qnHM",
    "outputId": "ac549336-a385-4b7c-c6e8-bafd1a19ebee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 2), (2000, 2), (8000,), (2000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "# checking for the train shape and test shape\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBbdm4GEri8-"
   },
   "source": [
    "#### c. To find good hyperparameters values for a DecisionTreeClassifier, use grid search with cross-validation (with the GridSearchCV class). Try different values for max leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shKi5LsVrpKG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import  DecisionTreeClassifier\n",
    "# setting up parameter\n",
    "params = {\n",
    "    'max_leaf_nodes': list(range(2, 100)),\n",
    "    'min_samples_split':[2, 3, 4],\n",
    "}\n",
    "\n",
    "# create instance of grid\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), \n",
    "                    params,\n",
    "                    verbose=5,\n",
    "                    cv=4)\n",
    "\n",
    "# fit the train data on grid search cv\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AknS6JKFshrx",
    "outputId": "a6a76622-884e-4a10-c200-63f7c1ea3295"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features=None, max_leaf_nodes=18,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                        random_state=42, splitter='best'),\n",
       " {'max_leaf_nodes': 18, 'min_samples_split': 2},\n",
       " 0.852)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking fot best result\n",
    "grid.best_estimator_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37xUsFBStafR",
    "outputId": "063f654a-32fb-4ffd-a509-735a9e89c9e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8375"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking on test set\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shFaPV-Dsslg"
   },
   "source": [
    "#### d. Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent.\n",
    "\n",
    "Yes after doing hyperparameter tuning i can achive arround 84% accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28qpjnI-tB5_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZiz7TNKtFMh"
   },
   "source": [
    "### 8. Follow these steps to grow a forest:\n",
    "\n",
    "* a. Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-ShuffleSplit Learn's class.\n",
    "\n",
    "* b. Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision        Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "\n",
    "* c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy's mode() function). Over the test collection, this method gives you majority-vote predictions.\n",
    "\n",
    "* d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approx 0.5 to 1.5 percent higher)\n",
    "\n",
    "* You've successfully learned a Random Forest classifier!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WdPmD5AAtOv_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "n_trees = 1000\n",
    "n_instances = 100\n",
    "\n",
    "mini_sets = []\n",
    "\n",
    "rs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)\n",
    "for mini_train_index, mini_test_index in rs.split(X_train):\n",
    "    X_mini_train = X_train[mini_train_index]\n",
    "    y_mini_train = y_train[mini_train_index]\n",
    "    mini_sets.append((X_mini_train, y_mini_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zkptW_rvGuE",
    "outputId": "bb2fccb3-7f53-4734-f98c-e92f61e6e73b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.786852"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "forest = [clone(grid.best_estimator_) for _ in range(n_trees)]\n",
    "\n",
    "accuracy_scores = []\n",
    "\n",
    "for tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets):\n",
    "    tree.fit(X_mini_train, y_mini_train)\n",
    "    \n",
    "    y_pred = tree.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "np.mean(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "m_rPQpoPvGq8"
   },
   "outputs": [],
   "source": [
    "Y_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8)\n",
    "\n",
    "for tree_index, tree in enumerate(forest):\n",
    "    Y_pred[tree_index] = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "txeZaTRGvWmE"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "y_pred_majority_votes, n_votes = mode(Y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gxFwsejvYPU",
    "outputId": "d1842a6c-8760-480d-9fae-0cdf7c7f5527"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8465"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_majority_votes.reshape([-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iymN7CBZvZwU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML - Assignment 21.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
