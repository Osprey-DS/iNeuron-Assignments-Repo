{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?\n",
    "Ans>> The key reason of reducing dimensionality is to reduce the complexity of the machine learning model. it can solve some storage problems, many high dimensionality data may need some time to reduce the dimension. It also may help in case of a multicollinearity problem. The major advantage of it can help the plot of the data in the 2d plane. And get a better understanding of the data.\n",
    "Although reducing the dimension of data did not lose any information but still there is a chance to lose some information when we performing dimensional reduction. It is like downgrade an image quality to a higher resolution to lower resolutions.  The dimensional reduction techniques may seep up the training process but they may give slightly worse performance. but some time in real-world problems it can be negligible, and this dimensional reduction technique is widely used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the dimensionality curse?\n",
    "Ans>>  The curse of dimensionality is meaning that we have so many features in our data or we have very high dimensional data, so the algorithm is failing to capture underlying pattern to build a model. also we have problem analyzing the data thoroughly due to may feature and it becomes very hard to visualize all of them for data analysis, this situation is called `Curse of Dimensionality`. Type of example can be Image data, wherewith height and width can produce high dimensional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about doing it? If not, what is the reason?\n",
    "Ans>> The dimensionality reduction techinuque is not reversable. Because when we perform dimension reduction technique, it loses some information there. And after this, there is no way It can go back it previous state correctly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?\n",
    "Ans>> PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scaler projection of the data comes to lie on the first coordinate that is called principal component, the second highest variance on the second coordinate, and so on. Many high diemensionality data have non linear nature. In this case the high dimensional data lie on or near a non-linear manifold and there fore PCA can not handle non-linear data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Assume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number of dimensions that the resulting dataset would have?\n",
    "Ans>> The number of dimensions is depend on the variation ratio and number of dimension and feature we have, it is very hard to say. It depends on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?\n",
    "Ans>> Vanilla PCA is is used when the data is fit on our system memory.\n",
    "Incremental PCA is used for larger dataset, when the full data is not fit our memory.\n",
    "Randomized PCA done the considerably reduce the dimensionality and the dataset fit the memory.\n",
    "Kernal Pca is used for non linear pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How do you assess a dimensionality reduction algorithm's success on your dataset?\n",
    "Ans>> We can create a base mode with using feature by not doing dimensionality reduction and can measure the performance of the base model and then create another model with performe the dimentionality reduction and measure the performance, we may notice is little or more better performance in second model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Is it logical to use two different dimensionality reduction algorithms in a chain?\n",
    "Ans>> Yes we can use different dimensionality algorithms as for needed for the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
