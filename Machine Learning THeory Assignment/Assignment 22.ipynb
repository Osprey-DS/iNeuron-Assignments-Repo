{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Is there any way to combine five different models that have all been trained on the same training data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is the reason?\n",
    "\n",
    "Ans>> This idea is called Ensemble learning, where we try to use multiple model to train same data instead of one model, to achive better performance. Ensemble is group of models that used together for prediction both classification and regression problems. By using this ensemble technique they reduce the bias means the underfitting issue, also lesser chance to overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What's the difference between hard voting classifiers and soft voting classifiers?\n",
    "\n",
    "Ans>> In case of hard voting classifier every classifier or model predict a class, and based on the majority vote for which class is get predicted by majority model, that is become final prediction.\n",
    "And in case of soft voting classifier every model gives output probability value instead of direct output label. And  the prediction are weighted by the classifierâ€™s importance and summed up. Then the target label with the greatest sum of weighted probabilites wins the vote and consider that is for final prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Is it possible to distribute a bagging ensemble's training through several servers to speed up the process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all options.\n",
    "Ans>> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the advantage of evaluating out of the bag?\n",
    "\n",
    "Ans>> In case of bootstarap sample there is one-third of data that was not used in the creation of model. it was out of sample and it is called out of  bag sample. With bagging some instance may be sampled serval times and other may not sampled at all. We can use this out-of-bag sample or oob for evaluating the model performance and it behave like unseen test data, the did not seen this during traning. This can be achive without need for separate validation set for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random Forests?\n",
    "\n",
    "Ans>> The random forest and extra tree classifier In case of random forest model each node only a random subset of features is considerd for splitting. But it is also possible to make tree even more random by also using random thresholds for each feature rather than searching for the best possible thresholds. In case of random forest it use bootstrap replicas, measn it subsample the data with replacement , where the extratree use the whole original sample. \n",
    "The Extratree classifier is lot faster than randomforest, because of it extra randomness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training data?\n",
    "Ans>> The AdaBoost is in `sklearn` `AdaBoostClassifier` which have `n_estimators` hyperparameter which control the how many Decision Stump is usning in the model creation time, we can regulate this number, if our model is underfit then we may want to increase this number or if our model is overfiting then we may want to reduce this number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the training set?\n",
    "Ans>> We always have to increase the learning rate if our model is overfit. Because if our leaning rate is too high then our model will fit too much on the traning set and it will not produced a generalize model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
